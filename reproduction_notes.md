## Environment used
1. Install Python. Version 3.9.2 is used.
2. Install Conda. Steps can be found in the "Install Conda" section of https://docs.google.com/document/d/1iuG6dNRAuhOU7K2ZeLNzIaV1w2InvMGq6VazBzZKFu4/.
3. In the `codes_for_models/zeroshot` folder,
    - run `conda env create -f env.yml`. (`env.yml` is based on the repo's original `env.yml`, but with matplotlib and more recent scipy)
    - run `pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz --no-deps`

## Testing with `logicedu.py` and `logicclimate.py`
In the `codes_for_models/experiments_round2` directory, the `threshold_testing.py` script will test saved models on the Logic and LogicClimate datasets, to try to reproduce the metrics stated in the paper (in Tables 5 and 7, respectively). Running `python threshold_testing.py` will run these evals for various threshold values (in determining if an input has a certain fallacy or not), print metrics for each threshold, and save these metrics to CSV files. It will also save raw predictions and labels, so that they don't have to be computed every time. Running `threshold_graphing.py` will, for each of these evaluations, generate plots of precision-vs.-recall curves for the different threshold values tested. Running `calibration_graphing.py` will, for each of the evaluations, generate calibration curves to see if the model is well-calibrated. Running `histogram_graphing.py` will, for each of these evaluations, generate histograms detailing the distributions of the (sigmoid) outputs of the model. The plots of `threshold_graphing.py`, `calibration_graphing.py`, and `histogram_graphing.py` are saved respectively in the `plots`, `calibration_plots`, and `distributions` folders in `codes_for_models/experiments_round2`.

By default, these scripts will use details in `evaluation_details/authors_saved.json` located in the same folder as the scripts. A different filename can be passed in as the first command-line argument into these scripts, and they will perform different evaluations:
- `evaluation_details/`
  - `authors_saved.json`: These will use the paper's authors' saved models. See "Obtaining saved models" to get those models in the right place for `threshold_testing.py` to use properly.
  - `authors_saved_by_fallacy.json`: These will perform some of the evaluations from `authors_saved.json`, except splitting up the evaluations by fallacy.
  - `authors_saved_eval_on_train.json`: These will perform some of the evaluations from `authors_saved.json`, except evaluating on the respective training dataset instead of the test set.
  - `authors_saved_microavg.json`: These will perform the same evaluations as `authors_saved.json`, except precisions and recalls are calculated using "micro" averaging instead of "samples" averaging. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) for more details about these averaging types.
  - `small_retrained.json`: These will use the models retrained from `electra-small-mnli`. See the filenames `train_electra_small_mnli_on_logic.json` and `finetune_trained_electra_small_logic_on_logicclimate.json` under the `training_details/` bullet point for getting the retrained models.
  - `small_20epochs_retrained.json`: These will use the models retrained from `electra-small-mnli` that took the best of 20 epochs instead of doing early stopping. See the filenames `train_electra_small_mnli_on_logic_20epochs.json` and `finetune_trained_electra_small_logic_on_logicclimate_20epochs.json` under the `training_details/` bullet point for getting the retrained models.
- `training_details/` (these are only meant to be passed into `threshold_testing.py`):
  - `train_electra_small_mnli_on_logic.json`: pass this file in to train a fresh `electra-small-mnli` model on the Logic dataset. See "Obtaining `electra-small-mnli` models for retraining" to get the fresh `electra-small-mnli` in the right place for `threshold_testing.py` to use properly.
  - `train_electra_small_mnli_on_logic.json`: pass this file in to train a fresh `electra-small-mnli` model on the Logic dataset. This training takes the best of 20 epochs instead of doing early stopping.
  - `finetune_trained_electra_small_logic_on_logicclimate.json`: pass this file in to finetune the models trained from `train_electra_small_mnli_on_logic.json`on the LogicClimate dataset.
  - `finetune_trained_electra_small_logic_on_logicclimate_20epochs.json`: pass this file in to finetune the models trained from `train_electra_small_mnli_on_logic.json` on the LogicClimate dataset. This finetuning takes the best of 20 epochs instead of doing early stopping.
### Obtaining saved models
- `threshold_testing.py` uses the saved models that the original repository provides. In the `README.md` file in the `saved_models` folder, download the saved models from the link, put them in the `saved_models` folder, then `unzip` them. The folder names corresponding to the saved models shouldn't have any of the numbers that were in the zip file names (i.e., it should just be "electra-logic", etc.).
- But, in the `codes_for_models/experiments_round2` folder, folders `raw_labels` and `raw_predictions` have been added, with the generated predictions along with labels used for evaluating the models. So, the saved models don't have to be downloaded if you're not doing re-evaluations of them.

#### Obtaining a `electra-small-mnli` model for retraining
To put the `electra-small-mnli` pretrained model in the correct place, run the `obtain_pretrained_seq_cls.py` script in `codes_for_models/experiments_round2`. This script saves a pretrained sequence classification model from `transformers`. The model name is passed in as the first command line argument, but the default is "`howey/electra-small-mnli`", the model this section concerns.

### Other notes
- For each input sample into the model, and for each of the 13 fallacies, the model will classify if the sample has the fallacy ("entailment"), doesn't ("contradiction"), or neutral ("neutral"). None of the training examples used in `logicedu.py` have the "neutral" label for any fallacy.
  - This makes the model more like 13 binary classifiers instead of a multi-class classifier.
  - By default, `logicedu.py` can't easily change threshold values (from the "threshold of 0.5") (since for each fallacy, the model outputs three numbers, and makes the classification based on which number is higheset). To be able to change threshold values, we subtract the "contradiction" output number from the "entailment" number. As a consequence, the "neutral" class no longer gets selected.
- In the dataset used in `logicclimate.py`, for some reason, there is exactly one input sample that doesn't have 13 fallacies associated with it, but only 10. This causes an error to occur in the `eval1` function, so this one example is excluded in our testing. Also, there seems to be one blank "example" each in the train, dev, and test datasets (i.e., line 10 in `data/climate_test_mh.csv`: "`10,10,,[],`") that causes errors in `logicclimate.py`, so that example is also excluded in our testing.
- A bug has been found where if you run `logicclimate.py` with a model already finetuned on the LogicClimate dataset (i.e. the `--finetune` flag unset/set to `F`), then the test set that the model gets evaluated on is the entire LogicClimate dataset (i.e., the train, dev, and test datasets). This repository has fixed this by adding the `--findtuned_model` flag to have already-finetuned models be evaluated on only the LogicClimate test dataset.