## Environment used
1. Install Python. Version 3.9.2 is used.
2. Install Conda. Steps can be found in the "Install Conda" section of [this document](https://docs.google.com/document/d/1iuG6dNRAuhOU7K2ZeLNzIaV1w2InvMGq6VazBzZKFu4/).
3. In the `codes_for_models/zeroshot` folder,
    - run `conda env create -f env.yml`. (`env.yml` is based on the repo's original `env.yml`, but with matplotlib, stanza, and sentence_transformers (and some more recent versions of libraries in the original `env.yml` for compatibility))
    - run `pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz --no-deps`

## Datasets used
The datasets used are the authors' Logic and LogicClimate datasets. These datasets are already in the repository in the (`data`) folder. Files starting with "edu" are in the Logic dataset, and files starting with "climate" are in the LogicClimate dataset. Which parts of the dataset are training, dev, and test sets are indicated by the file name too. For the LogicClimate dataset, files with "mh" are the main ones used since they have the logical-masked forms of examples, though files without "mh" contain URLs to where the example is originally found.

## Testing with `logicedu.py` and `logicclimate.py`
In the `codes_for_models/experiments_round2` directory, the authors' `logicedu.py` and `logicclimate.py` files evaluate a given model on the Logic and LogicClimate dataset, respectively, and caluclate metrics based on those evaluations. We've made some changes to these files in this repository to make the code work with ours, and to be able to perform evaluations for different threshold values. Also in that directory, our `threshold_testing.py` script will test saved models on the Logic and LogicClimate datasets, to try to reproduce the metrics stated in the paper (in Tables 5 and 7, respectively). Running `python threshold_testing.py` will run these evals for various threshold values (in determining if an input has a certain fallacy or not), print metrics for each threshold, and save these metrics to CSV files. It will also save raw predictions and labels and load them if they're already saved, so that they don't have to be computed every time. `threshold_testing.py` runs the evals for various thresholds by default, but they can optionally run the evals for just the 0.5 threshold (i.e. what's originally used); pass "T" as the second command-line argument into this script to do this. Note that most of the functionality of `threshold_testing.py` is from invoking and passing command line arguments into the `logicedu.py`/`logicclimate.py`.

Running `threshold_graphing.py` will, for each of these evaluations, generate plots of precision-vs.-recall curves for the different threshold values tested (each threshold represented by a point on the graph). Running `calibration_graphing.py` will, for each of the evaluations, generate calibration curves to see if the model is well-calibrated. Running `histogram_graphing.py` will, for each of these evaluations, generate histograms detailing the distributions of the (sigmoid) outputs of the model. The plots of `threshold_graphing.py`, `calibration_graphing.py`, and `histogram_graphing.py` are saved respectively in the `threshold_plots`, `calibration_plots`, and `distribution_histograms` folders in `codes_for_models/experiments_round2`. More details of these plots are in "Where/how results are stored".

By default, these scripts (including `threshold_testing.py`) will use details in `evaluation_details/authors_saved.json` located in the same folder as the scripts. A different filename can be passed in as the first command-line argument into these scripts, and they will perform different evaluations:
- `evaluation_details/`
  - `authors_saved.json`: These will use the paper's authors' saved models. See "Obtaining saved models" to get those models in the right place for `threshold_testing.py` to use properly.
  - `authors_saved_by_fallacy.json`: These will perform some of the evaluations from `authors_saved.json`, except splitting up the evaluations by fallacy.
  - `authors_saved_eval_on_train.json`: These will perform some of the evaluations from `authors_saved.json`, except evaluating on the respective training dataset instead of the test set.
  - `authors_saved_microavg.json`: These will perform the same evaluations as `authors_saved.json`, except precisions and recalls are calculated using "micro" averaging instead of "samples" averaging. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) for more details about these averaging types.
  - `small_retrained.json`: These will use the models retrained from `electra-small-mnli`. See the filenames `train_electra_small_mnli_on_logic.json` and `finetune_trained_electra_small_logic_on_logicclimate.json` under the `training_details/` bullet point for getting the retrained models.
  - `small_20epochs_retrained.json`: These will use the models retrained from `electra-small-mnli` that took the best of 20 epochs instead of doing early stopping. See the filenames `train_electra_small_mnli_on_logic_20epochs.json` and `finetune_trained_electra_small_logic_on_logicclimate_20epochs.json` under the `training_details/` bullet point for getting the retrained models.

## Retraining models
The `model_training.py` script will perform some model (re)trainings based on what training details file you pass in (whose filename is passed in as the first command-line argument). The script's default training details file is `training_details/train_electra_small_mnli_on_logic.json`. Note that most of the functionality of `model_training.py` is from invoking and passing command line arguments into `logicedu.py`/`logicclimate.py`. These are some of the filenames that can be passed into `model_training.py`:
- `training_details/`
  - `train_electra_small_mnli_on_logic.json`: pass this file in to train a fresh pretrained `electra-small-mnli` model on the Logic dataset. A non-StructAware model and a StructAware model will be trained. See "Obtaining `electra-small-mnli` models for retraining" to get the fresh `electra-small-mnli` in the right place for `threshold_testing.py` to use properly.
  - `train_electra_small_mnli_on_logic_20epochs.json`: does the same training as `train_electra_small_mnli_on_logic.json`. Except, this training takes the best of 20 epochs instead of doing early stopping (taking the epoch before the first epoch where val loss doesn't decrease).
  - `finetune_trained_electra_small_logic_on_logicclimate.json`: pass this file in to finetune the models trained from `train_electra_small_mnli_on_logic.json`on the LogicClimate dataset.
  - `finetune_trained_electra_small_logic_on_logicclimate_20epochs.json`: does the same finetuning as `finetune_trained_electra_small_logic_on_logicclimate.json`. Except, this training takes the best of 20 epochs instead of doing early stopping (taking the epoch before the first epoch where val loss doesn't decrease).

### Where/how results are stored
The results of experiments are stored in various directories in `codes_for_models/experiments_round2`:
- `calibration_plots` is where plots generated by `calibration_graphing.py` are stored. For a plot, examples on the dataset are sorted based on the model's sigmoid output and placed into some number of buckets. Each data point on the calibration plot represents one of the buckets. For each data point/bucket, the average output sigmoid value of the bucket (which is supposed to represent the average calculated probability of an example in the bucket having some given fallacy) (x) is plotted against the fraction of examples in that bucket whose true label is positive (that they have some given fallacy) (which is supposed to represent the actual probability) (y). If the model is well-calibrated, x should be approximately equal to y.
- `distribution_histograms` is where plots generated by `histogram_graphing.py` are stored. For a plot, there's some set number of fixed-size bins from 0 to 1, and examples in the dataset are put into bins based on the model's sigmoid output of that example. The histogram plots the number of examples in each bucket.
- `metrics` is where various scoring metrics are stored for a model's evaluation on a dataset, for different threshold values. The model is evaluated on the dataset using the threshold value to classify if a given example has a given fallacy or not. Then based on how the model evaluated the dataset, micro f1, macro f1, precision, recall, exact match (how often the model got everything correct about an example), and accuracy are calculated. This file is outputted from `logicedu.py`/`logicclimate.py`.
- `raw_labels` is where the true labels for the dataset, that a given model is evaluated on, is stored. The rows, columns, and values of labels correspond to those of the corresponding file (with the same name and relative path) in `raw_predictions`. 1 means that the example has the fallacy; 0 means it doesn't. This file is outputted from `logicedu.py`/`logicclimate.py`.
- `raw_predictions` is where a given model's predictions on a given dataset are stored, as a saved PyTorch tensor. Rows are the different examples in the dataset, and columns are the model's (non-sigmoid) outputs of that example for a specific fallacy. See "Other notes" for more details on these output numbers. This file is outputted from `logicedu.py`/`logicclimate.py`.
- `threshold_plots` is where plots generated by `threshold_graphing.py` are stored. For a plot, each data point represents a different threshold value which the model is evaluated at. For each threshold/data point, the model evaluates each example in the dataset being evaluated on, using that threshold to determine if an example has a given fallacy (thresholds can range from 0 to 1, where 0 means the model always predicts the example as having the fallacy, and 1 means it's always predicted to not have the fallacy). The precision (y) and recall (x) of these evaluations are plotted. If the paper has a corresponding precision and recall to compare with the model's actual precisions/recalls, then that too will be included as a data point for comparison.

#### File/directory name abbreviations in `threshold_plots`, etc. folders
The results in "Where/how results are stored" are organized into different subdirectories within these directories. These are some abbreviations on what certain directory names and parts of file names mean:
- `authors_saved`: Uses the authors' saved models
- `small`: Uses models retrained from `electra-small-mnli`
- `eval_train_set`: Evaluation on training set instead of test set
- `micro_avg`: Precisions and recalls calculated using "micro" averaging instead of the default "samples" averaging
- `on-logic`: Evaluation on the Logic dataset 
- `on-lclimate`: Evaluation on the LogicClimate dataset 
- `saware`: Model evaluated is a StructAware model
- `ftolc`: Model evaluated was **f**ine**t**uned **o**n the **L**ogic**C**limate dataset
- `ts3`: "Training strategy 3", where the model was trained on both original ("Training strategy 1") articles and masked ("Training strategy 2") articles

## Obtaining saved models
- `threshold_testing.py` uses the saved models that the original repository provides. In the `README.md` file in the `saved_models` folder, download the saved models from the link, put them in the `saved_models` folder, then `unzip` them. The folder names corresponding to the saved models shouldn't have any of the numbers that were in the zip file names (i.e., it should just be "electra-logic", etc.).
- But, in the `codes_for_models/experiments_round2` folder, folders `raw_labels` and `raw_predictions` have been added, with the generated predictions along with labels used for evaluating the models. So, the saved models don't have to be downloaded if you're not doing re-evaluations of them.

## Obtaining pretrained models for retraining
The models that can be used for retraining are `electra-small-mnli` (smaller) and `howey/electra-base-mnli` (larger). To put the `electra-small-mnli` pretrained model in the correct place, run the `obtain_pretrained_seq_cls.py` script in `codes_for_models/experiments_round2`. This script saves a pretrained sequence classification model from `transformers`. The model name is passed in as the first command line argument, but the default is "`howey/electra-small-mnli`". If you want to obtain an `electra-base-mnli` pretrained model, pass in `howey/electra-base-mnli` as the command line argument.

## Where data is preprocessed
In the `codes_for_models/experiments_round2` directory, the authors' `remove_content_words.py` will input a file name and mask out certain content words in sentences in that file, to get a more logical form of the sentence. Their script accepts a CSV filename, the name of the column in that CSV file containing the sentences, and the name of the model to be used for processing the sentences. Names of pretrained models that can be used can be found [here](https://www.sbert.net/docs/pretrained_models.html)

## Other notes
- For each input sample into the model, and for each of the 13 fallacies, the model will classify if the sample has the fallacy ("entailment"), doesn't ("contradiction"), or neutral ("neutral"). None of the training examples used in `logicedu.py` have the "neutral" label for any fallacy.
  - This makes the model more like 13 binary classifiers instead of a multi-class classifier.
  - By default, `logicedu.py` can't easily change threshold values (from the "threshold of 0.5") (since for each fallacy, the model outputs three numbers, and makes the classification based on which number is higheset). To be able to change threshold values, we subtract the "contradiction" output number from the "entailment" number. As a consequence, the "neutral" class no longer gets selected.
- In the dataset used in `logicclimate.py`, for some reason, there is exactly one input sample that doesn't have 13 fallacies associated with it, but only 10. This causes an error to occur in the `eval1` function, so this one example is excluded in our testing. Also, there seems to be one blank "example" each in the train, dev, and test datasets (i.e., line 10 in `data/climate_test_mh.csv`: "`10,10,,[],`") that causes errors in `logicclimate.py`, so that example is also excluded in our testing.
- A bug has been found where if you run `logicclimate.py` with a model already finetuned on the LogicClimate dataset (i.e. the `--finetune` flag unset/set to `F`), then the test set that the model gets evaluated on is the entire LogicClimate dataset (i.e., the train, dev, and test datasets). This repository has fixed this by adding the `--findtuned_model` flag to have already-finetuned models be evaluated on only the LogicClimate test dataset.

## Convenience
Two convenience scripts are in the `codes_for_models/experiments_round2` folder:
- `all_training.py` properly obtains a fresh pretrained `electra-small-mnli` dataset, and makes/saves various trained versions of it to be used by `threshold_testing.py` (if "`evaluation_details/small_retrained.json`" or "`evaluation_details/small_20epochs_retrained.json`" is passed in as the command line argument (string)).
- `all_evals_and_graphing.py` performs evaluations for all saved models concerned by files in `evaluation_details`, then plots precision/recall curves, etc. for each evaluation.

**To regenerate all saved predictions/labels, evaluation metrics, and plots**, first follow the steps in "Obtaining saved models", then run `all_training.py`, then run `all_evals_and_graphing.py`
**To simply evaluate the authors' saved models (without thresholds)**, first follow the steps in "Obtaining saved models" (if not using the saved predictions/labels), then (in `codes_for_models/experiments_round2`) run `python threshold_testing.py evaluation_details/authors_saved.json T`.